# -*- coding: utf-8 -*-
"""model 1.1 learn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sIiL4tLtwOqTsnAX-SUd3wIhS7fHAXbw
"""

import os  # Работа с файловой системой
import tarfile  # Работа с архивами tar
import requests  # HTTP-запросы для скачивания данных (ссылка)
import torch  # Библиотека для работы с нейросетями
from torch.utils.data import Dataset, DataLoader  # Создание кастомных датасетов и загрузчиков данных
from torchvision import transforms  # Преобразование изображений
from PIL import Image  # Работа с изображениями
from xml.etree import ElementTree as ET  # Разбор XML-аннотаций (рамки и метки)
from transformers import AutoImageProcessor, AutoModelForObjectDetection  # Модели из библиотеки transformers
from tqdm import tqdm  # Прогресс-бары (доп инфа)
import torch.optim as optim  # Оптимизаторы для обучения модели
from pycocotools.coco import COCO  # Работа с разметкой в формате COCO
from pycocotools.cocoeval import COCOeval  # Оценка модели по COCO-метрикам
import multiprocessing  # Работа с многопроцессорностью (ускорение)

# Функция для скачивания и распаковки данных PASCAL VOC
def download_and_extract_voc(url, folder_name):
    if not os.path.exists(folder_name):  # Проверяем, существует ли папка
        os.makedirs(folder_name)  # Если нет, создаем

    tar_path = os.path.join(folder_name, url.split('/')[-1])  # Формируем путь к tar-файлу

    if not os.path.exists(tar_path):  # Если архив ещё не скачан
        print(f"Downloading {url}...")
        with requests.get(url, stream=True) as r:  # Загружаем файл потоково (частями)
            r.raise_for_status()  # Проверяем на ошибки HTTP
            with open(tar_path, 'wb') as f:  # Открываем файл для записи в бинарном режиме
                for chunk in r.iter_content(chunk_size=8192):  # Читаем по 8192 байта (оптимально)
                    f.write(chunk)  # Записываем в файл

    if tarfile.is_tarfile(tar_path):  # Проверяем, является ли файл архивом tar
        print(f"Extracting {tar_path}...")
        with tarfile.open(tar_path, 'r') as tar_ref:  # Открываем tar-архив
            tar_ref.extractall(folder_name)  # Распаковываем

# Скачиваем и распаковываем VOC 2012
download_and_extract_voc('http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar', '/content/voc2012')

# Список классов VOC и их индексы
class_names = {name: i + 1 for i, name in enumerate([
    'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow',
    'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'
])}  # Словарь, где ключ – имя класса, значение – индекс

# Определяем класс датасета для загрузки изображений и аннотаций VOC
class VOCDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.image_dir = os.path.join(root_dir, 'VOCdevkit', 'VOC2012', 'JPEGImages')  # Путь к изображениям
        self.ann_dir = os.path.join(root_dir, 'VOCdevkit', 'VOC2012', 'Annotations')  # Путь к аннотациям
        self.image_ids = [f.split('.')[0] for f in os.listdir(self.image_dir)]  # Получаем список ID изображений
        self.transform = transform  # Преобразования, если заданы

    def __len__(self):
        return len(self.image_ids)  # Возвращаем количество изображений в датасете

    def __getitem__(self, idx):
        img_id = self.image_ids[idx]  # Получаем ID изображения
        image = Image.open(os.path.join(self.image_dir, f'{img_id}.jpg')).convert("RGB")  # Загружаем изображение

        tree = ET.parse(os.path.join(self.ann_dir, f'{img_id}.xml'))  # Загружаем XML-аннотацию
        boxes, labels = [], []  # Списки для хранения координат объектов и их классов

        for obj in tree.findall('object'):  # Перебираем объекты в XML
            class_name = obj.find('name').text  # Получаем имя класса объекта
            if class_name in class_names:  # Если класс есть в списке
                bbox = obj.find('bndbox')  # Достаем координаты ограничивающего прямоугольника
                boxes.append([float(bbox.find(tag).text) for tag in ('xmin', 'ymin', 'xmax', 'ymax')])  # Записываем координаты
                labels.append(class_names[class_name])  # Записываем индекс класса

        if self.transform:  # Если заданы преобразования
            image = self.transform(image)  # Применяем их к изображению

        return image, torch.tensor(boxes, dtype=torch.float32), torch.tensor(labels, dtype=torch.int64)  # Возвращаем изображение, боксы и метки классов

# Функция для вычисления mAP
def evaluate_map(model, dataloader):
    model.eval()  # Переводим модель в режим оценки (отключаем обновление весов) те без кл изменений
    coco_gt = COCO()  # Инициализация объекта для хранения аннотаций COCO
    coco_dt = []  # Список для хранения предсказаний модели в формате COCO

    with torch.no_grad():  # Отключаем градиенты, чтобы ускорить вычисления
        for images, _, _ in tqdm(dataloader, desc='Evaluating'):  # Проходим по батчам данных
            images = torch.stack(images).to(device, non_blocking=True)  # Переносим изображения на GPU/CPU (на процессор)
            inputs = processor(images=images, return_tensors="pt").to(device)  # Обрабатываем изображения
            outputs = model(**inputs)  # Прогоняем изображения через модель

            for logits, boxes in zip(outputs.logits, outputs.pred_boxes):  # Перебираем логи и боксы (вероятность и рамки)
                pred_scores = torch.nn.functional.softmax(logits, dim=-1)  # Применяем softmax для вероятностей
                pred_labels = pred_scores.argmax(dim=-1)  # Получаем метки классов (с максимальной вероятностью)
                pred_boxes = boxes.cpu().numpy()  # Переводим боксы на CPU и в numpy формат

                for i in range(len(pred_labels)):  # Для каждого объекта в предсказании
                    if pred_labels[i] != len(class_names):  # Исключаем фон (класс с максимальным индексом)
                        coco_dt.append({  # Добавляем предсказание в список COCO
                            "image_id": i,
                            "category_id": pred_labels[i].item(),  # Класс
                            "bbox": pred_boxes[i].tolist(),  # Координаты бокса
                            "score": pred_scores[i].max().item(),  # Оценка уверенности
                        })

    coco_eval = COCOeval(coco_gt, coco_dt, "bbox")  # Инициализация оценки COCO для боксов
    coco_eval.evaluate()  # Оценка модели
    coco_eval.accumulate()  # Накопление статистики
    coco_eval.summarize()  # Вывод результатов

    return coco_eval.stats[0]  # Возвращаем mAP (первый элемент статистики)


# Функция для объединения батчей (collate_fn)
def collate_fn(batch):
    images, boxes, labels = zip(*batch)  # Разделяем батч на изображения, боксы и метки
    images = [img for img in images]  # Оставляем изображения в списке (не изменяем)
    max_boxes = max([len(b) for b in boxes])  # Находим максимальное количество боксов на изображение

    # Дополняем боксы до максимального размера
    padded_boxes = [torch.cat([b, torch.zeros(max_boxes - len(b), 4)]) for b in boxes]
    # Дополняем метки до максимального размера
    padded_labels = [torch.cat([l, torch.zeros(max_boxes - len(l), dtype=torch.int64)]) for l in labels]

    # Объединяем боксы и метки в тензоры
    boxes = torch.stack(padded_boxes)
    labels = torch.stack(padded_labels)

    return images, boxes, labels  # Возвращаем изображения, боксы и метки


# Определяем устройство (GPU или CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Инициализация модели
model_name = "hustvl/yolos-tiny"  # Название модели
model = AutoModelForObjectDetection.from_pretrained(model_name).to(device)  # Загружаем модель для детекции объектов
processor = AutoImageProcessor.from_pretrained(model_name)  # Загружаем процессор для обработки изображений

# Подготовка датасета
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Уменьшаем размер изображения для ускорения
    transforms.ToTensor(),
])

# Создание объекта датасета
train_dataset = VOCDataset(root_dir='/content/voc2012', transform=transform)

# Определение количества потоков для загрузки данных
num_workers = min(4, multiprocessing.cpu_count())  # Уменьшаем количество потоков
train_loader = DataLoader(
    train_dataset,
    batch_size=4,  # Уменьшаем размер батча для ускорения
    collate_fn=collate_fn,
    shuffle=True,
    num_workers=num_workers,
    pin_memory=True
)

# Оценка оригинальной модели на меньшем наборе данных
map_before = evaluate_map(model, train_loader)
print(f"mAP before training: {map_before}")

# Обучение модели
optimizer = optim.AdamW(model.parameters(), lr=1e-5)
scaler = GradScaler()  # Инициализация скейлера для смешанной точности
num_epochs = 1  # Сохраняем 1 эпоху для ускорения

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for images, boxes, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):
        images = torch.stack(images).to(device, non_blocking=True)

        optimizer.zero_grad()

        with autocast():  # Включаем автоматическую точность с плавающей запятой
            # Используем processor с do_rescale=False
            inputs = processor(images=images, return_tensors="pt", do_rescale=False).to(device)
            outputs = model(**inputs)
            loss = outputs.losses["labels"]

        # Используем GradScaler для управления процессом обновления градиентов
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        running_loss += loss.item()
    print(f"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}")

# Сохранение модели
model.save_pretrained("/content/yolos_finetuned")
processor.save_pretrained("/content/yolos_finetuned")

# Оценка дообученной модели
map_after = evaluate_map(model, train_loader)
print(f"mAP after training: {map_after}")